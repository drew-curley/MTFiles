{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the language model pretrained file\n",
    "\n",
    "#!wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin\n",
    "!pip install fasttext\n",
    "\n",
    "import fasttext\n",
    "\n",
    "pretrained_lang_model = \"./lid218e.bin\" # path of pretrained model file\n",
    "model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "text = \"صباح الخير، الجو جميل اليوم والسماء صافية.\"\n",
    "predictions = model.predict(text, k=1)\n",
    "print(predictions)\n",
    "input_lang = predictions[0][0].replace('__label__', '')\n",
    "print(input_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-docx\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import glob\n",
    "from pathlib import Path, PurePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'facebook/nllb-200-3.3B'\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test sentence tokenizer:\n",
    "from nltk import sent_tokenize\n",
    "para = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target languages and corresponding file names\n",
    "languages = [\n",
    "    (\"amh_Ethi\", \"Ethiopian\"),\n",
    "    (\"arb_Arab\", \"Arabic\"),\n",
    "    (\"asm_Beng\", \"Assamese\"),\n",
    "    (\"ben_Beng\", \"Bangal\"),\n",
    "    (\"por_Latn\", \"BPortugese\"),\n",
    "    (\"mya_Mymr\", \"Burmese\"),\n",
    "    (\"ceb_Latn\", \"Cebuano\"),\n",
    "    (\"zsm_Latn\", \"Chinese\"),\n",
    "    (\"fra_Latn\", \"French\"),\n",
    "    (\"guj_Gujr\", \"Gujarati\"),\n",
    "    (\"hau_Latn\", \"Hausa\"),\n",
    "    (\"hin_Deva\", \"Hindi\"),\n",
    "    (\"ilo_Latn\", \"Illocano\"),\n",
    "    (\"ind_Latn\", \"Indonesian\"),\n",
    "    (\"kan_Knda\", \"Kannada\"),\n",
    "    (\"khm_Khmr\", \"Khmer\"),\n",
    "    (\"lao_Laoo\", \"Laotian\"),\n",
    "    (\"spa_Latn\", \"LASpanish\"),\n",
    "    (\"mal_Mlym\", \"Malayalam\"),\n",
    "    (\"npi_Deva\", \"Nepali\"),\n",
    "    (\"ory_Orya\", \"Oriya\"),\n",
    "    (\"plt_Latn\", \"PlatMalagasy\"),\n",
    "    (\"pan_Guru\", \"EPunjabi\"),\n",
    "    (\"rus_Cyrl\", \"Russian\"),\n",
    "    (\"swh_Latn\", \"Swahili\"),\n",
    "    (\"tgl_Latn\", \"Tagalog\"),\n",
    "    (\"tam_Taml\", \"Tamil\"),\n",
    "    (\"tel_Telu\", \"Telugu\"),\n",
    "    (\"tha_Thai\", \"Thai\"),\n",
    "    (\"tpi_Latn\", \"TokPisin\"),\n",
    "    (\"urd_Arab\", \"Urdu\"),\n",
    "    (\"vie_Latn\", \"Vietnamese\")\n",
    "]\n",
    "\n",
    "files = [file for file in input_folder.rglob(\"*.\" + ext_in)]\n",
    "print(f\"Found {len(files)} {ext_in} files in {input_folder.resolve()}\")\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "    file = file.resolve()\n",
    "    languages_in_file = get_languages(file)\n",
    "    top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "    file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "\n",
    "    if file_is_english:\n",
    "        print(f\"{i:>4} : Translating file {file} from English to multiple languages.\")\n",
    "        try:\n",
    "            document = docx.Document(file)\n",
    "        except BadZipFile:\n",
    "            print(f\"BadZipFile Error on opening {file}\")\n",
    "            continue\n",
    "\n",
    "        for target_lang, file_name in languages:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            translation_pipeline = pipeline('translation',\n",
    "                                            model=model,\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            src_lang='eng_Latn',\n",
    "                                            tgt_lang=target_lang,\n",
    "                                            max_length=400,\n",
    "                                            device=device)\n",
    "\n",
    "            paragraphs = translate_docx(file)\n",
    "            \n",
    "            output_path = output_folder / f\"{file.stem}_{file_name}.{ext_out}\"\n",
    "            save_translated_document(paragraphs, output_path)\n",
    "            \n",
    "            print(f\"{i:>4} : Translated file {file} to {file_name}.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"{i:>4} : Not translating file {file}. It seems to be in :{top_language_in_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = Path(\"home/curleyd/GitHub/MTFiles\")\n",
    "output_folder = Path(\"home/curleyd/GitHub/MTFiles/Translated\")\n",
    "ext_in = 'docx'\n",
    "ext_out = 'docx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs_from_docx(file):\n",
    "\n",
    "    paras = []\n",
    "    # Open connection to Word Document\n",
    "    doc = docx.Document(file)\n",
    "\n",
    "    # read in each paragraph in file and store the style name with it.\n",
    "    for para in doc.paragraphs:\n",
    "        this_para = {'style': para.style.name}\n",
    "        sentences = []\n",
    "        for sentence in sent_tokenize(para.text):\n",
    "            sentences.append(sentence)\n",
    "        this_para['sentences'] = sentences\n",
    "\n",
    "    #print(f'Found {len(styles_in_doc)} styles {styles_in_doc} in this document.')\n",
    "    return paras\n",
    "\n",
    "import shutil\n",
    "def translate_docx(file):\n",
    "    \n",
    "    paras = []\n",
    "    # Copy file to output folder and then work off of the copy\n",
    "    file = shutil.copy(file, output_folder)\n",
    "\n",
    "    # Open connection to Word Document\n",
    "    doc = docx.Document(file)\n",
    "\n",
    "    # read in each paragraph in file and store the style name with it.\n",
    "    for para in doc.paragraphs:\n",
    "        this_para = {'style': para.style.name}\n",
    "        sentences = [sentence for sentence in sent_tokenize(para.text)]\n",
    "        translations = [translation_pipeline(sentence)[0]['translation_text'] for sentence in sentences]\n",
    "\n",
    "        this_para['sentences'] = sentences\n",
    "        this_para['translations'] = translations\n",
    "        paras.append(this_para)\n",
    "\n",
    "        # This line was a great simplification of the find and replace code.\n",
    "        para.text = \" \".join(translations)\n",
    "\n",
    "        # I'm not sure this is required, since the style shouldn't have changed.\n",
    "        para.style = this_para['style']\n",
    "\n",
    "\n",
    "        #print(this_para)\n",
    "\n",
    "    doc.save(file)\n",
    "    #print(f'Found {len(styles_in_doc)} styles {styles_in_doc} in this document.')\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "pretrained_lang_model = \"./lid218e.bin\" # path of pretrained model file\n",
    "fasttext_model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "def get_languages(file):\n",
    "\n",
    "    file = file.resolve()\n",
    "\n",
    "    #print(f\"Opening {file}\")\n",
    "    # Open the input file as a Word document\n",
    "    try :\n",
    "        document = docx.Document(file)\n",
    "    except BadZipFile:\n",
    "        print(f\"BadZipFile Error on opening {file}\")\n",
    "\n",
    "    paragraphs = [para for para in document.paragraphs]\n",
    "    sentences = [sentence for para in document.paragraphs for sentence in sent_tokenize(para.text)]\n",
    "\n",
    "    languages = Counter()\n",
    "    for sentence in sentences:\n",
    "        predictions = fasttext_model.predict(sentence, k=1)\n",
    "        #print(predictions)\n",
    "        output_lang = predictions[0][0].replace('__label__', '')\n",
    "        #print(output_lang)\n",
    "        languages.update([output_lang])\n",
    "\n",
    "    return languages\n",
    "\n",
    "\n",
    "def show_languages(files):\n",
    "    results = {}\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        file = file.resolve()\n",
    "        languages = get_languages(file)\n",
    "\n",
    "        #print(languages)\n",
    "        #print(f\"There are {len(sentences)} sentences in {output_file}.\")\n",
    "        results[file] = {\"languages\": languages, \"sentence_count\": len(sentences)}\n",
    "\n",
    "    for file, info in results.items():\n",
    "        print(f\"There are {info['sentence_count']} sentences in {str(file)[23:]}    {info['languages'].most_common()}\")\n",
    "        #print(f\"{info['languages']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in input_folder.rglob(\"*.\" + ext_in)]\n",
    "print(f\"Found {len(files)} {ext_in} files in {input_folder.resolve()}\")\n",
    "\n",
    "for i, file in enumerate(files,1):\n",
    "    file = file.resolve()\n",
    "    languages_in_file = get_languages(file)\n",
    "    top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "    file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "\n",
    "    #print(top_language_in_file, file_is_english)\n",
    "\n",
    "    if file_is_english:\n",
    "        print(f\"{i:>4} : Translating file {file} from English to Spanish.\")\n",
    "        try :\n",
    "            document = docx.Document(file)\n",
    "        except BadZipFile:\n",
    "            print(f\"BadZipFile Error on opening {file}\")\n",
    "            continue\n",
    "\n",
    "        # Save the file.\n",
    "        document.save(file)\n",
    "\n",
    "        # Translate the content\n",
    "        paragraphs = translate_docx(file)\n",
    "\n",
    "        print(f\"{i:>4} : Translated file {file} from English to Spanish\")\n",
    "    else:\n",
    "        print(f\"{i:>4} : Not translating file {file}. It seems to be in :{top_language_in_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
