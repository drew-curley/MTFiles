{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing Necessary Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext\n",
    "!pip install -U pip transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-docx\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Additional Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import fasttext\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import docx\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from nltk import sent_tokenize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Language Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained language model\n",
    "pretrained_lang_model = \"./lid218e.bin\"  # path of the pretrained model file\n",
    "# Check if the file exists\n",
    "if not os.path.isfile(pretrained_lang_model):\n",
    "    # If the file doesn't exist, download it\n",
    "    !wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin\n",
    "fasttext_model = fasttext.load_model(pretrained_lang_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting Up Translation Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the translation model checkpoint and load the model and tokenizer\n",
    "checkpoint = 'facebook/nllb-200-3.3B'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Target Language and File Names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target languages and corresponding file names\n",
    "languages = [\n",
    "    (\"amh_Ethi\", \"Ethiopian\"),\n",
    "    (\"arb_Arab\", \"Arabic\"),\n",
    "    (\"asm_Beng\", \"Assamese\"),\n",
    "    (\"ben_Beng\", \"Bangal\"),\n",
    "    (\"por_Latn\", \"BPortugese\"),\n",
    "    (\"mya_Mymr\", \"Burmese\"),\n",
    "    (\"ceb_Latn\", \"Cebuano\"),\n",
    "    (\"zsm_Latn\", \"Chinese\"),\n",
    "    (\"fra_Latn\", \"French\"),\n",
    "    (\"guj_Gujr\", \"Gujarati\"),\n",
    "    (\"hau_Latn\", \"Hausa\"),\n",
    "    (\"hin_Deva\", \"Hindi\"),\n",
    "    (\"ilo_Latn\", \"Illocano\"),\n",
    "    (\"ind_Latn\", \"Indonesian\"),\n",
    "    (\"kan_Knda\", \"Kannada\"),\n",
    "    (\"khm_Khmr\", \"Khmer\"),\n",
    "    (\"lao_Laoo\", \"Laotian\"),\n",
    "    (\"spa_Latn\", \"LASpanish\"),\n",
    "    (\"mal_Mlym\", \"Malayalam\"),\n",
    "    (\"npi_Deva\", \"Nepali\"),\n",
    "    (\"ory_Orya\", \"Oriya\"),\n",
    "    (\"plt_Latn\", \"PlatMalagasy\"),\n",
    "    (\"pan_Guru\", \"EPunjabi\"),\n",
    "    (\"rus_Cyrl\", \"Russian\"),\n",
    "    (\"swh_Latn\", \"Swahili\"),\n",
    "    (\"tgl_Latn\", \"Tagalog\"),\n",
    "    (\"tam_Taml\", \"Tamil\"),\n",
    "    (\"tel_Telu\", \"Telugu\"),\n",
    "    (\"tha_Thai\", \"Thai\"),\n",
    "    (\"tpi_Latn\", \"TokPisin\"),\n",
    "    (\"urd_Arab\", \"Urdu\"),\n",
    "    (\"vie_Latn\", \"Vietnamese\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Function to Get Languages from DOCX Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages(file_path):\n",
    "    # Function to detect languages used in the document\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    \n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "        \n",
    "    text = ' '.join(full_text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    languages = Counter()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        prediction = fasttext_model.predict(sentence, k=1)\n",
    "        lang = prediction[0][0].replace('__label__', '')\n",
    "        languages.update([lang])\n",
    "    \n",
    "    return languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translate DOCX File Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_docx(file_path):\n",
    "    \"\"\"\n",
    "    Function to translate DOCX file content from English to Spanish\n",
    "    \"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    translated_paragraphs = []\n",
    "    \n",
    "    for para in doc.paragraphs:\n",
    "        input_text = para.text\n",
    "        translated_text = translate_text(input_text)\n",
    "        translated_paragraphs.append(translated_text)\n",
    "    \n",
    "    # Save the translated paragraphs back to the document\n",
    "    for para, translated_text in zip(doc.paragraphs, translated_paragraphs):\n",
    "        para.text = translated_text\n",
    "    \n",
    "    doc.save(file_path)\n",
    "\n",
    "def translate_text(text, source_lang='eng_Latn', target_lang='spa_Latn'):\n",
    "    # Function to translate text using the loaded translation model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing Files in the Input Folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the input folder and file extension\n",
    "input_folder = Path(\"./Input\")\n",
    "ext_in = \"docx\"\n",
    "\n",
    "# Get the list of files to process\n",
    "files = [file for file in input_folder.rglob(\"*.\" + ext_in)]\n",
    "\n",
    "# Process each file\n",
    "results = {}\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "    file = file.resolve()\n",
    "    languages_in_file = get_languages(file)\n",
    "    top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "    file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "    \n",
    "    if file_is_english:\n",
    "        try:\n",
    "            document = docx.Document(file)\n",
    "        except BadZipFile:\n",
    "            print(f\"BadZipFile Error on opening {file}\")\n",
    "            continue\n",
    "        \n",
    "        # Save the file\n",
    "        document.save(file)\n",
    "        \n",
    "        # Translate the content\n",
    "        paragraphs = translate_docx(file)\n",
    "        \n",
    "        print(f\"{i:>4} : Translated file {file} from English to Spanish\")\n",
    "    else:\n",
    "        print(f\"{i:>4} : Not translating file {file}. It seems to be in: {top_language_in_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
