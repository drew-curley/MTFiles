{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 1: Language Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in ./venv2/lib/python3.12/site-packages (0.9.3)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./venv2/lib/python3.12/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./venv2/lib/python3.12/site-packages (from fasttext) (75.2.0)\n",
      "Requirement already satisfied: numpy in ./venv2/lib/python3.12/site-packages (from fasttext) (1.26.4)\n",
      "Using GPU: NVIDIA GeForce RTX 3090\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mصباح الخير، الجو جميل اليوم والسماء صافية.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Predict the language of the text\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mfasttext_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Extract and print the language code\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/MTFiles/venv2/lib/python3.12/site-packages/fasttext/FastText.py:239\u001b[0m, in \u001b[0;36m_FastText.predict\u001b[0;34m(self, text, k, threshold, on_unicode_error)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     probs, labels \u001b[38;5;241m=\u001b[39m ([], ())\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labels, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword."
     ]
    }
   ],
   "source": [
    "# Download the fastText language model and install the required package\n",
    "!pip install fasttext\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import fasttext\n",
    "\n",
    "# check if cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available, using CPU instead.\")\n",
    "\n",
    "# Load the pre-trained language model\n",
    "pretrained_lang_model = \"./lid218e.bin\"  # path of the pretrained model file\n",
    "# Check if the file exists\n",
    "if not os.path.isfile(pretrained_lang_model):\n",
    "    # If the file doesn't exist, download it\n",
    "    !wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin\n",
    "fasttext_model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "# Sample text for language detection\n",
    "text = \"صباح الخير، الجو جميل اليوم والسماء صافية.\"\n",
    "\n",
    "# Predict the language of the text\n",
    "predictions = fasttext_model.predict(text, k=1)\n",
    "print(predictions)\n",
    "\n",
    "# Extract and print the language code\n",
    "input_lang = predictions[0][0].replace('__label__', '')\n",
    "print(input_lang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 2: Installing Necessary Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages for tokenization and translation\n",
    "!pip install -U pip transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-docx\n",
    "!pip install nltk\n",
    "\n",
    "# Import NLTK and download required data\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 3: Importing Additional Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for document processing\n",
    "import docx\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 4:Setting Up Translation Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the translation model checkpoint and load the model and tokenizer\n",
    "checkpoint = 'facebook/nllb-200-3.3B'\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 5: Testing Sentence Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 6: Defining Target Language and File Names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target languages and corresponding file names\n",
    "languages = [\n",
    "    # (\"amh_Ethi\", \"Ethiopian\"),\n",
    "    # (\"arb_Arab\", \"Arabic\"),\n",
    "    # (\"asm_Beng\", \"Assamese\"),\n",
    "    # (\"ben_Beng\", \"Bangal\"),\n",
    "    # (\"por_Latn\", \"BPortugese\"),\n",
    "    # (\"mya_Mymr\", \"Burmese\"),\n",
    "    # (\"ceb_Latn\", \"Cebuano\"),\n",
    "    # (\"zsm_Latn\", \"Chinese\"),\n",
    "    # (\"fra_Latn\", \"French\"),\n",
    "    # (\"guj_Gujr\", \"Gujarati\"),\n",
    "    # (\"hau_Latn\", \"Hausa\"),\n",
    "    # (\"hin_Deva\", \"Hindi\"),\n",
    "    # (\"ilo_Latn\", \"Illocano\"),\n",
    "    # (\"ind_Latn\", \"Indonesian\"),\n",
    "    # (\"kan_Knda\", \"Kannada\"),\n",
    "    # (\"khm_Khmr\", \"Khmer\"),\n",
    "    # (\"lao_Laoo\", \"Laotian\"),\n",
    "    (\"spa_Latn\", \"LASpanish\"),\n",
    "    # (\"mal_Mlym\", \"Malayalam\"),\n",
    "    # (\"npi_Deva\", \"Nepali\"),\n",
    "    # (\"ory_Orya\", \"Oriya\"),\n",
    "    # (\"plt_Latn\", \"PlatMalagasy\"),\n",
    "    # (\"pan_Guru\", \"EPunjabi\"),\n",
    "    # (\"rus_Cyrl\", \"Russian\"),\n",
    "    # (\"swh_Latn\", \"Swahili\"),\n",
    "    # (\"tgl_Latn\", \"Tagalog\"),\n",
    "    # (\"tam_Taml\", \"Tamil\"),\n",
    "    # (\"tel_Telu\", \"Telugu\"),\n",
    "    # (\"tha_Thai\", \"Thai\"),\n",
    "    # (\"tpi_Latn\", \"TokPisin\"),\n",
    "    # (\"urd_Arab\", \"Urdu\"),\n",
    "    # (\"vie_Latn\", \"Vietnamese\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 7: Helper Function to Get Languages from DOCX Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_languages(file_path):\n",
    "    # Function to detect languages used in the document\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    \n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "        \n",
    "    text = ' '.join(full_text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    languages = Counter()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        prediction = fasttext_model.predict(sentence, k=1)\n",
    "        lang = prediction[0][0].replace('__label__', '')\n",
    "        languages.update([lang])\n",
    "    \n",
    "    return languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 8: Translate DOCX File Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_docx(file_path):\n",
    "    \"\"\"\n",
    "    Function to translate DOCX file content from English to Spanish\n",
    "    \"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    translated_paragraphs = []\n",
    "    \n",
    "    for para in doc.paragraphs:\n",
    "        input_text = para.text\n",
    "        translated_text = translate_text(input_text)\n",
    "        translated_paragraphs.append(translated_text)\n",
    "    \n",
    "    # Save the translated paragraphs back to the document\n",
    "    for para, translated_text in zip(doc.paragraphs, translated_paragraphs):\n",
    "        para.text = translated_text\n",
    "    \n",
    "    doc.save(file_path)\n",
    "\n",
    "def translate_text(text, source_lang='eng_Latn', target_lang='spa_Latn'):\n",
    "    # Function to translate text using the loaded translation model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    \n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 9: Processing Files in the Input Folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the input folder and file extension\n",
    "input_folder = Path(\"/home/drew/Documents/GitHub/MTFiles/Input\")\n",
    "ext_in = \"docx\"\n",
    "\n",
    "# Get the list of files to process\n",
    "files = [file for file in input_folder.rglob(\"*.\" + ext_in)]\n",
    "\n",
    "print(f\"Found {len(files)} {ext_in} files in {input_folder.resolve()}\")\n",
    "\n",
    "# Process each file\n",
    "results = {}\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "    file = file.resolve()\n",
    "    languages_in_file = get_languages(file)\n",
    "    top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "    file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "    \n",
    "    if file_is_english:\n",
    "        print(f\"{i:>4} : Translating file {file} from English to Spanish.\")\n",
    "        try:\n",
    "            document = docx.Document(file)\n",
    "        except BadZipFile:\n",
    "            print(f\"BadZipFile Error on opening {file}\")\n",
    "            continue\n",
    "        \n",
    "        # Save the file\n",
    "        document.save(file)\n",
    "        \n",
    "        # Translate the content\n",
    "        paragraphs = translate_docx(file)\n",
    "        \n",
    "        print(f\"{i:>4} : Translated file {file} from English to Spanish\")\n",
    "    else:\n",
    "        print(f\"{i:>4} : Not translating file {file}. It seems to be in: {top_language_in_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
