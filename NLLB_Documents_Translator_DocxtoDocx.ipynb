{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 1: Language Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fasttext in /home/curleyd/.local/lib/python3.10/site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in /home/curleyd/.local/lib/python3.10/site-packages (from fasttext) (1.26.4)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/curleyd/.local/lib/python3.10/site-packages (from fasttext) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/lib/python3/dist-packages (from fasttext) (59.6.0)\n",
      "Using GPU: NVIDIA RTX A6000\n",
      "(('__label__arb_Arab',), array([0.99960977]))\n",
      "arb_Arab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Download the fastText language model and install the required package\n",
    "!pip install fasttext\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import fasttext\n",
    "\n",
    "# check if cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available, using CPU instead.\")\n",
    "\n",
    "# Load the pre-trained language model\n",
    "pretrained_lang_model = \"./lid218e.bin\"  # path of the pretrained model file\n",
    "# Check if the file exists\n",
    "if not os.path.isfile(pretrained_lang_model):\n",
    "    # If the file doesn't exist, download it\n",
    "    !wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin\n",
    "fasttext_model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "# Sample text for language detection\n",
    "text = \"صباح الخير، الجو جميل اليوم والسماء صافية.\"\n",
    "\n",
    "# Predict the language of the text\n",
    "predictions = fasttext_model.predict(text, k=1)\n",
    "print(predictions)\n",
    "\n",
    "# Extract and print the language code\n",
    "input_lang = predictions[0][0].replace('__label__', '')\n",
    "print(input_lang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 2: Installing Necessary Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/curleyd/.local/lib/python3.10/site-packages (24.1.2)\n",
      "Requirement already satisfied: transformers in /home/curleyd/.local/lib/python3.10/site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/curleyd/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/curleyd/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/curleyd/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in /home/curleyd/.local/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/curleyd/.local/lib/python3.10/site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/curleyd/.local/lib/python3.10/site-packages (from python-docx) (4.11.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/curleyd/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/curleyd/.local/lib/python3.10/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/curleyd/.local/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/curleyd/.local/lib/python3.10/site-packages (from nltk) (4.66.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/curleyd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary packages for tokenization and translation\n",
    "!pip install -U pip transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-docx\n",
    "!pip install nltk\n",
    "\n",
    "# Import NLTK and download required data\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 3: Importing Additional Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for document processing\n",
    "import docx\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 4:Setting Up Translation Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 10.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the translation model checkpoint and load the model and tokenizer\n",
    "checkpoint = 'facebook/nllb-200-3.3B'\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 5: Testing Sentence Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for helping Drew.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test sentence tokenizer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Sample paragraph for tokenization\n",
    "para = \"Hello World. It's good to see you. Thanks for helping Drew.\"\n",
    "sent_tokenize(para)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 6: Defining Target Language and File Names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target languages and corresponding file names\n",
    "languages = [\n",
    "    (\"amh_Ethi\", \"Ethiopian\"),\n",
    "    (\"arb_Arab\", \"Arabic\"),\n",
    "    (\"asm_Beng\", \"Assamese\"),\n",
    "    (\"ben_Beng\", \"Bangal\"),\n",
    "    (\"por_Latn\", \"BPortugese\"),\n",
    "    (\"mya_Mymr\", \"Burmese\"),\n",
    "    (\"ceb_Latn\", \"Cebuano\"),\n",
    "    (\"zsm_Latn\", \"Chinese\"),\n",
    "    (\"fra_Latn\", \"French\"),\n",
    "    (\"guj_Gujr\", \"Gujarati\"),\n",
    "    (\"hau_Latn\", \"Hausa\"),\n",
    "    (\"hin_Deva\", \"Hindi\"),\n",
    "    (\"ilo_Latn\", \"Illocano\"),\n",
    "    (\"ind_Latn\", \"Indonesian\"),\n",
    "    (\"kan_Knda\", \"Kannada\"),\n",
    "    (\"khm_Khmr\", \"Khmer\"),\n",
    "    (\"lao_Laoo\", \"Laotian\"),\n",
    "    (\"spa_Latn\", \"LASpanish\"),\n",
    "    (\"mal_Mlym\", \"Malayalam\"),\n",
    "    (\"npi_Deva\", \"Nepali\"),\n",
    "    (\"ory_Orya\", \"Oriya\"),\n",
    "    (\"plt_Latn\", \"PlatMalagasy\"),\n",
    "    (\"pan_Guru\", \"EPunjabi\"),\n",
    "    (\"rus_Cyrl\", \"Russian\"),\n",
    "    (\"swh_Latn\", \"Swahili\"),\n",
    "    (\"tgl_Latn\", \"Tagalog\"),\n",
    "    (\"tam_Taml\", \"Tamil\"),\n",
    "    (\"tel_Telu\", \"Telugu\"),\n",
    "    (\"tha_Thai\", \"Thai\"),\n",
    "    (\"tpi_Latn\", \"TokPisin\"),\n",
    "    (\"urd_Arab\", \"Urdu\"),\n",
    "    (\"vie_Latn\", \"Vietnamese\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 7: Helper Function to Get Languages from DOCX Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_languages(file_path):\n",
    "    # Function to detect languages used in the document\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    \n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "        \n",
    "    text = ' '.join(full_text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    languages = Counter()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        prediction = model.predict(sentence, k=1)\n",
    "        lang = prediction[0][0].replace('__label__', '')\n",
    "        languages.update([lang])\n",
    "    \n",
    "    return languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 8: Translate DOCX File Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_docx(file_path):\n",
    "    \"\"\"\n",
    "    Function to translate DOCX file content from English to Spanish\n",
    "    \"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    translated_paragraphs = []\n",
    "    \n",
    "    for para in doc.paragraphs:\n",
    "        input_text = para.text\n",
    "        translated_text = translate_text(input_text)\n",
    "        translated_paragraphs.append(translated_text)\n",
    "    \n",
    "    # Save the translated paragraphs back to the document\n",
    "    for para, translated_text in zip(doc.paragraphs, translated_paragraphs):\n",
    "        para.text = translated_text\n",
    "    \n",
    "    doc.save(file_path)\n",
    "\n",
    "def translate_text(text, source_lang='eng_Latn', target_lang='spa_Latn'):\n",
    "    # Function to translate text using the loaded translation model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    \n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell 9: Processing Files in the Input Folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 docx files in /home/curleyd/GitHub/MTFiles\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'M2M100ForConditionalGeneration' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(files, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     16\u001b[0m     file \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[0;32m---> 17\u001b[0m     languages_in_file \u001b[38;5;241m=\u001b[39m \u001b[43mget_languages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     top_language_in_file \u001b[38;5;241m=\u001b[39m languages_in_file\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m     file_is_english \u001b[38;5;241m=\u001b[39m top_language_in_file \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng_Latn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[27], line 17\u001b[0m, in \u001b[0;36mget_languages\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m languages \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m---> 17\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(sentence, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m     lang \u001b[38;5;241m=\u001b[39m prediction[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__label__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m     languages\u001b[38;5;241m.\u001b[39mupdate([lang])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'M2M100ForConditionalGeneration' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the input folder and file extension\n",
    "input_folder = Path(\"/home/curleyd/GitHub/MTFiles\")\n",
    "ext_in = \"docx\"\n",
    "\n",
    "# Get the list of files to process\n",
    "files = [file for file in input_folder.rglob(\"*.\" + ext_in)]\n",
    "\n",
    "print(f\"Found {len(files)} {ext_in} files in {input_folder.resolve()}\")\n",
    "\n",
    "# Process each file\n",
    "results = {}\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "    file = file.resolve()\n",
    "    languages_in_file = get_languages(file)\n",
    "    top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "    file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "    \n",
    "    if file_is_english:\n",
    "        print(f\"{i:>4} : Translating file {file} from English to Spanish.\")\n",
    "        try:\n",
    "            document = docx.Document(file)\n",
    "        except BadZipFile:\n",
    "            print(f\"BadZipFile Error on opening {file}\")\n",
    "            continue\n",
    "        \n",
    "        # Save the file\n",
    "        document.save(file)\n",
    "        \n",
    "        # Translate the content\n",
    "        paragraphs = translate_docx(file)\n",
    "        \n",
    "        print(f\"{i:>4} : Translated file {file} from English to Spanish\")\n",
    "    else:\n",
    "        print(f\"{i:>4} : Not translating file {file}. It seems to be in: {top_language_in_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
