{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curleyd/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# run off of GPU in PyTorch\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# check if cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-16 11:08:09--  https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.173.166.74, 18.173.166.48, 18.173.166.51, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.173.166.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1176355829 (1.1G) [application/octet-stream]\n",
      "Saving to: ‘lid218e.bin.1’\n",
      "\n",
      "lid218e.bin.1       100%[===================>]   1.09G  85.3MB/s    in 13s     \n",
      "\n",
      "2024-04-16 11:08:22 (84.9 MB/s) - ‘lid218e.bin.1’ saved [1176355829/1176355829]\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fasttext in /home/curleyd/.local/lib/python3.10/site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in /home/curleyd/.local/lib/python3.10/site-packages (from fasttext) (1.26.4)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/curleyd/.local/lib/python3.10/site-packages (from fasttext) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/lib/python3/dist-packages (from fasttext) (59.6.0)\n",
      "(('__label__arb_Arab',), array([0.99960977]))\n",
      "arb_Arab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# download the language model pretrained file\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin\n",
    "!pip install fasttext\n",
    "\n",
    "import fasttext\n",
    "\n",
    "pretrained_lang_model = \"./lid218e.bin\" # path of pretrained model file\n",
    "model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "text = \"صباح الخير، الجو جميل اليوم والسماء صافية.\"\n",
    "predictions = model.predict(text, k=1)\n",
    "print(predictions)\n",
    "input_lang = predictions[0][0].replace('__label__', '')\n",
    "print(input_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/curleyd/.local/lib/python3.10/site-packages (24.0)\n",
      "Requirement already satisfied: transformers in /home/curleyd/.local/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/curleyd/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/curleyd/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/curleyd/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in /home/curleyd/.local/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/curleyd/.local/lib/python3.10/site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/curleyd/.local/lib/python3.10/site-packages (from python-docx) (4.11.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/curleyd/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/curleyd/.local/lib/python3.10/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/curleyd/.local/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/curleyd/.local/lib/python3.10/site-packages (from nltk) (4.66.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/curleyd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U pip transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-docx\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import glob\n",
    "from pathlib import Path, PurePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smallest 600M parameter model - distilled\n",
    "# checkpoint = 'facebook/nllb-200-distilled-600M'\n",
    "\n",
    "# Medium 1.3B parameter model - distilled\n",
    "# checkpoint = 'facebook/nllb-200-distilled-1.3B'\n",
    "\n",
    "# Medium 1.3B parameter model\n",
    "# checkpoint = 'facebook/nllb-200-1.3B'\n",
    "\n",
    "# Large 3.3B parameter model\n",
    "checkpoint = 'facebook/nllb-200-3.3B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test sentence tokenizer:\n",
    "from nltk import sent_tokenize\n",
    "para = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "हेलो वर्ल्ड, तपाईंलाई भेटेर खुसी लाग्यो, यो पुस्तक किनेकोमा धन्यवाद।\n"
     ]
    }
   ],
   "source": [
    "input_lang = 'eng_Latn'\n",
    "target_lang = 'npi_Deva'\n",
    "translation_pipeline = pipeline('translation',\n",
    "                                model=model,\n",
    "                                tokenizer=tokenizer,\n",
    "                                src_lang=input_lang,\n",
    "                                tgt_lang=target_lang,\n",
    "                                max_length = 400)\n",
    "output = translation_pipeline(para)\n",
    "print(output[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs_from_docx(file):\n",
    "\n",
    "    paras = []\n",
    "    # Open connection to Word Document\n",
    "    doc = docx.Document(file)\n",
    "\n",
    "    # read in each paragraph in file and store the style name with it.\n",
    "    for para in doc.paragraphs:\n",
    "        this_para = {'style': para.style.name}\n",
    "        sentences = []\n",
    "        for sentence in sent_tokenize(para.text):\n",
    "            sentences.append(sentence)\n",
    "        this_para['sentences'] = sentences\n",
    "\n",
    "    #print(f'Found {len(styles_in_doc)} styles {styles_in_doc} in this document.')\n",
    "    return paras\n",
    "\n",
    "\n",
    "def translate_docx(file):\n",
    "\n",
    "    paras = []\n",
    "\n",
    "    # Open connection to Word Document\n",
    "    doc = docx.Document(file)\n",
    "\n",
    "    # read in each paragraph in file and store the style name with it.\n",
    "    for para in doc.paragraphs:\n",
    "        this_para = {'style': para.style.name}\n",
    "        sentences = [sentence for sentence in sent_tokenize(para.text)]\n",
    "        translations = [translation_pipeline(sentence)[0]['translation_text'] for sentence in sentences]\n",
    "\n",
    "        this_para['sentences'] = sentences\n",
    "        this_para['translations'] = translations\n",
    "        paras.append(this_para)\n",
    "\n",
    "        # This line was a great simplification of the find and replace code.\n",
    "        para.text = \" \".join(translations)\n",
    "\n",
    "        # I'm not sure this is required, since the style shouldn't have changed.\n",
    "        para.style = this_para['style']\n",
    "\n",
    "\n",
    "        #print(this_para)\n",
    "\n",
    "    doc.save(file)\n",
    "    #print(f'Found {len(styles_in_doc)} styles {styles_in_doc} in this document.')\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang = 'eng_Latn'\n",
    "output_lang = 'npi_Deva'\n",
    "\n",
    "translation_pipeline2 = pipeline('translation',\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        src_lang=input_lang,\n",
    "                        tgt_lang=output_lang,\n",
    "                        max_length = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = Path(\"/home/curleyd/pytorch_stuff/Downloads/Choosingaspouse\")\n",
    "output_folder = Path(\"/home/curleyd/pytorch_stuff/Downloads/Choosingaspousenepali\")\n",
    "ext_in = 'docx'\n",
    "ext_out = 'docx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "pretrained_lang_model = \"./lid218e.bin\" # path of pretrained model file\n",
    "fasttext_model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "def get_languages(file):\n",
    "\n",
    "    file = file.resolve()\n",
    "\n",
    "    #print(f\"Opening {file}\")\n",
    "    # Open the input file as a Word document\n",
    "    try :\n",
    "        document = docx.Document(file)\n",
    "    except BadZipFile:\n",
    "        print(f\"BadZipFile Error on opening {file}\")\n",
    "\n",
    "    paragraphs = [para for para in document.paragraphs]\n",
    "    sentences = [sentence for para in document.paragraphs for sentence in sent_tokenize(para.text)]\n",
    "\n",
    "    languages = Counter()\n",
    "    for sentence in sentences:\n",
    "        predictions = fasttext_model.predict(sentence, k=1)\n",
    "        #print(predictions)\n",
    "        output_lang = predictions[0][0].replace('__label__', '')\n",
    "        #print(output_lang)\n",
    "        languages.update([output_lang])\n",
    "\n",
    "    return languages\n",
    "\n",
    "\n",
    "def show_languages(files):\n",
    "    results = {}\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        file = file.resolve()\n",
    "        languages = get_languages(file)\n",
    "\n",
    "        #print(languages)\n",
    "        #print(f\"There are {len(sentences)} sentences in {output_file}.\")\n",
    "        results[file] = {\"languages\": languages, \"sentence_count\": len(sentences)}\n",
    "\n",
    "    for file, info in results.items():\n",
    "        print(f\"There are {info['sentence_count']} sentences in {str(file)[23:]}    {info['languages'].most_common()}\")\n",
    "        #print(f\"{info['languages']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 docx files in /home/curleyd/pytorch_stuff/Downloads\n",
      "   1 : Not translating file /home/curleyd/pytorch_stuff/Downloads/Honorandrepsect.docx. It seems to be in :npi_Deva.\n",
      "   2 : Not translating file /home/curleyd/pytorch_stuff/Downloads/Biblestudyquestions.docx. It seems to be in :npi_Deva.\n",
      "   3 : Not translating file /home/curleyd/pytorch_stuff/Downloads/Hope.docx. It seems to be in :npi_Deva.\n",
      "   4 : Not translating file /home/curleyd/pytorch_stuff/Downloads/Stepstowardsfreedom.docx. It seems to be in :npi_Deva.\n",
      "   5 : Not translating file /home/curleyd/pytorch_stuff/Downloads/CommunicatingwithJesus.docx. It seems to be in :npi_Deva.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m file \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m      7\u001b[0m languages_in_file \u001b[38;5;241m=\u001b[39m get_languages(file)\n\u001b[0;32m----> 8\u001b[0m top_language_in_file \u001b[38;5;241m=\u001b[39m \u001b[43mlanguages_in_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_common\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m file_is_english \u001b[38;5;241m=\u001b[39m top_language_in_file \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng_Latn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#print(top_language_in_file, file_is_english)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "folder = Path(\"/home/curleyd/pytorch_stuff/Downloads/\")\n",
    "files = [file for file in folder.rglob(\"*.\" + ext_in)]\n",
    "print(f\"Found {len(files)} {ext_in} files in {folder.resolve(True)}\")\n",
    "\n",
    "for i, file in enumerate(files,1):\n",
    "    file = file.resolve()\n",
    "    languages_in_file = get_languages(file)\n",
    "    top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "    file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "\n",
    "    #print(top_language_in_file, file_is_english)\n",
    "\n",
    "    if file_is_english:\n",
    "        print(f\"{i:>4} : Translating file {file} from English to Spanish.\")\n",
    "        try :\n",
    "            document = docx.Document(file)\n",
    "        except BadZipFile:\n",
    "            print(f\"BadZipFile Error on opening {file}\")\n",
    "            continue\n",
    "\n",
    "        # Save the file.\n",
    "        document.save(file)\n",
    "\n",
    "        # Translate the content\n",
    "        paragraphs = translate_docx(file)\n",
    "\n",
    "        print(f\"{i:>4} : Translated file {file} from English to Spanish\")\n",
    "    else:\n",
    "        print(f\"{i:>4} : Not translating file {file}. It seems to be in :{top_language_in_file}.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
