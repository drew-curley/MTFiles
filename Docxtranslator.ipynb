{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "# run off of GPU in PyTorch\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# check if cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fasttext in /home/curleyd/.local/lib/python3.10/site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in /home/curleyd/.local/lib/python3.10/site-packages (from fasttext) (1.26.4)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/curleyd/.local/lib/python3.10/site-packages (from fasttext) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/lib/python3/dist-packages (from fasttext) (59.6.0)\n",
      "Model already exists. Skipping download.\n",
      "(('__label__arb_Arab',), array([0.99960977]))\n",
      "arb_Arab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# download the language model pretrained file\n",
    "\n",
    "!pip install fasttext\n",
    "\n",
    "import fasttext\n",
    "import os\n",
    "\n",
    "pretrained_lang_model = \"./lid218e.bin\" # path of pretrained model file\n",
    "if not os.path.exists(pretrained_lang_model):\n",
    "    !wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin\n",
    "else:\n",
    "    print(\"Model already exists. Skipping download.\")\n",
    "    \n",
    "model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "text = \"صباح الخير، الجو جميل اليوم والسماء صافية.\"\n",
    "predictions = model.predict(text, k=1)\n",
    "print(predictions)\n",
    "test_lang = predictions[0][0].replace('__label__', '')\n",
    "print(test_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/curleyd/.local/lib/python3.10/site-packages (24.0)\n",
      "Requirement already satisfied: transformers in /home/curleyd/.local/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/curleyd/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/curleyd/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/curleyd/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/curleyd/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in /home/curleyd/.local/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/curleyd/.local/lib/python3.10/site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/curleyd/.local/lib/python3.10/site-packages (from python-docx) (4.11.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/curleyd/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/curleyd/.local/lib/python3.10/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/curleyd/.local/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/curleyd/.local/lib/python3.10/site-packages (from nltk) (4.66.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/curleyd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U pip transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-docx\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import glob\n",
    "from pathlib import Path, PurePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "def use_nllb():\n",
    "    # Smallest 600M parameter model - distilled\n",
    "    # checkpoint = 'facebook/nllb-200-distilled-600M'\n",
    "\n",
    "    # Medium 1.3B parameter model - distilled\n",
    "    # checkpoint = 'facebook/nllb-200-distilled-1.3B'\n",
    "\n",
    "    # Medium 1.3B parameter model\n",
    "    # checkpoint = 'facebook/nllb-200-1.3B'\n",
    "\n",
    "    # Large 3.3B parameter model\n",
    "    checkpoint = 'facebook/nllb-200-3.3B'\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    return (model, tokenizer)\n",
    "\n",
    "def use_madlad():\n",
    "    madlad_model=\"google/madlad400-10b-mt\"\n",
    "\n",
    "    pipe = pipeline(\"translation\", model=madlad_model)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(madlad_model)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(madlad_model)\n",
    "    return (model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Model to use (comment out which one you do __not__ want to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 9/9 [00:09<00:00,  1.06s/it]\n",
      "Loading checkpoint shards:  44%|████▍     | 4/9 [00:05<00:06,  1.27s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# model, tokenizer = use_nllb()\n",
    "model, tokenizer = use_madlad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test sentence tokenizer:\n",
    "from nltk import sent_tokenize\n",
    "para = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = 'eng_Latn'\n",
    "target_language = 'npi_Deva'\n",
    "\n",
    "translation_pipeline = pipeline('translation',\n",
    "                                model=model,\n",
    "                                tokenizer=tokenizer,\n",
    "                                src_lang=source_language,\n",
    "                                tgt_lang=target_language,\n",
    "                                max_length = 400)\n",
    "output = translation_pipeline(para)\n",
    "print(output[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs_from_docx(file):\n",
    "\n",
    "    paras = []\n",
    "    # Open connection to Word Document\n",
    "    doc = docx.Document(file)\n",
    "\n",
    "    # read in each paragraph in file and store the style name with it.\n",
    "    for para in doc.paragraphs:\n",
    "        this_para = {'style': para.style.name}\n",
    "        sentences = []\n",
    "        for sentence in sent_tokenize(para.text):\n",
    "            sentences.append(sentence)\n",
    "        this_para['sentences'] = sentences\n",
    "\n",
    "    #print(f'Found {len(styles_in_doc)} styles {styles_in_doc} in this document.')\n",
    "    return paras\n",
    "\n",
    "\n",
    "def translate_docx(file):\n",
    "\n",
    "    paras = []\n",
    "\n",
    "    # Open connection to Word Document\n",
    "    doc = docx.Document(file)\n",
    "\n",
    "    # read in each paragraph in file and store the style name with it.\n",
    "    for para in doc.paragraphs:\n",
    "        this_para = {'style': para.style.name}\n",
    "        sentences = [sentence for sentence in sent_tokenize(para.text)]\n",
    "        translations = [translation_pipeline(sentence)[0]['translation_text'] for sentence in sentences]\n",
    "\n",
    "        this_para['sentences'] = sentences\n",
    "        this_para['translations'] = translations\n",
    "        paras.append(this_para)\n",
    "\n",
    "        # This line was a great simplification of the find and replace code.\n",
    "        para.text = \" \".join(translations)\n",
    "\n",
    "        # I'm not sure this is required, since the style shouldn't have changed.\n",
    "        para.style = this_para['style']\n",
    "\n",
    "\n",
    "        #print(this_para)\n",
    "\n",
    "    doc.save(file)\n",
    "    #print(f'Found {len(styles_in_doc)} styles {styles_in_doc} in this document.')\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_pipeline2 = pipeline('translation',\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        src_lang=source_language,\n",
    "                        tgt_lang=target_language,\n",
    "                        max_length = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "input_folder = Path(\"/home/curleyd/pytorch_stuff/Downloads/Choosingaspouse\")\n",
    "output_folder = Path(\"/home/curleyd/pytorch_stuff/Downloads/Choosingaspousenepali\")\n",
    "ext_in = 'docx'\n",
    "ext_out = 'docx'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Iterate over files in the input folder and copy the docx files to output\n",
    "for file_path in input_folder.glob(\"*\"):\n",
    "    if file_path.suffix == \".docx\":\n",
    "        destination_path = output_folder / file_path.name\n",
    "        shutil.copy(file_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "pretrained_lang_model = \"./lid218e.bin\" # path of pretrained model file\n",
    "fasttext_model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "def get_languages(file):\n",
    "\n",
    "    file = file.resolve()\n",
    "\n",
    "    #print(f\"Opening {file}\")\n",
    "    # Open the input file as a Word document\n",
    "    try :\n",
    "        document = docx.Document(file)\n",
    "    except BadZipFile:\n",
    "        print(f\"BadZipFile Error on opening {file}\")\n",
    "\n",
    "    paragraphs = [para for para in document.paragraphs]\n",
    "    sentences = [sentence for para in document.paragraphs for sentence in sent_tokenize(para.text)]\n",
    "\n",
    "    languages = Counter()\n",
    "    for sentence in sentences:\n",
    "        predictions = fasttext_model.predict(sentence, k=1)\n",
    "        #print(predictions)\n",
    "        output_lang = predictions[0][0].replace('__label__', '')\n",
    "        #print(output_lang)\n",
    "        languages.update([output_lang])\n",
    "\n",
    "    return languages\n",
    "\n",
    "\n",
    "def show_languages(files):\n",
    "    results = {}\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        file = file.resolve()\n",
    "        languages = get_languages(file)\n",
    "\n",
    "        #print(languages)\n",
    "        #print(f\"There are {len(sentences)} sentences in {output_file}.\")\n",
    "        results[file] = {\"languages\": languages, \"sentence_count\": len(sentences)}\n",
    "\n",
    "    for file, info in results.items():\n",
    "        print(f\"There are {info['sentence_count']} sentences in {str(file)[23:]}    {info['languages'].most_common()}\")\n",
    "        #print(f\"{info['languages']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from English\n",
    "def translate_from_english(output_folder, ext_in):\n",
    "    folder = output_folder  # Path(\"/home/curleyd/pytorch_stuff/Downloads/\")\n",
    "    files = [file for file in folder.rglob(\"*.\" + ext_in)]\n",
    "    print(f\"Found {len(files)} {ext_in} files in {folder.resolve(True)}\")\n",
    "\n",
    "    for i, file in enumerate(files, 1):\n",
    "        file = file.resolve()\n",
    "        languages_in_file = get_languages(file)\n",
    "        if languages_in_file:\n",
    "            top_languages = languages_in_file.most_common(1)\n",
    "            if top_languages:\n",
    "                top_language_in_file = top_languages[0][0]\n",
    "            else:\n",
    "                top_language_in_file = \"eng_Latn\"\n",
    "        else:\n",
    "            top_language_in_file = \"eng_Latn\"\n",
    "\n",
    "        file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "\n",
    "        if file_is_english:\n",
    "            print(f\"{i:>4} : Translating file {file} from English to Nepali.\")\n",
    "            try:\n",
    "                document = docx.Document(file)\n",
    "            except BadZipFile:\n",
    "                print(f\"BadZipFile Error on opening {file}\")\n",
    "                continue\n",
    "\n",
    "            # Save the file.\n",
    "            document.save(file)\n",
    "\n",
    "            # Translate the content\n",
    "            paragraphs = translate_docx(file)\n",
    "\n",
    "            print(f\"{i:>4} : Translated file {file} from English to Nepali\")\n",
    "        else:\n",
    "            print(f\"{i:>4} : Not translating file {file}. It seems to be in :{top_language_in_file}.\")\n",
    "\n",
    "\n",
    "### Into English\n",
    "def translate_to_english(output_folder, ext_in):\n",
    "    folder = output_folder  # Path(\"/home/curleyd/pytorch_stuff/Downloads/darrell/darrell/darrell/\")\n",
    "    files = [file for file in folder.rglob(\"*.\" + ext_in)]\n",
    "    print(f\"Found {len(files)} {ext_in} files in {folder.resolve()}\")\n",
    "\n",
    "    for i, file in enumerate(files, 1):\n",
    "        file = file.resolve()\n",
    "        languages_in_file = get_languages(file)\n",
    "        top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "        file_is_source_language = top_language_in_file == source_language\n",
    "\n",
    "        if file_is_source_language:\n",
    "            print(f\"{i:>4} : Translating file {file} from {source_language} to {target_language}.\")\n",
    "            try:\n",
    "                document = docx.Document(file)\n",
    "            except BadZipFile:\n",
    "                print(f\"BadZipFile Error on opening {file}\")\n",
    "                continue\n",
    "\n",
    "            # Translate the content\n",
    "            paragraphs = translate_docx(file)\n",
    "\n",
    "            print(f\"{i:>4} : Translated file {file} from {source_language} to {target_language}\")\n",
    "        else:\n",
    "            print(f\"{i:>4} : Not translating file {file}. It seems to be in :{top_language_in_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if source_language == \"eng_Latn\":\n",
    "    translate_from_english(output_folder=output_folder, ext_in=ext_in)\n",
    "elif target_language == \"eng_Latn\":\n",
    "    translate_to_english(output_folder=output_folder, ext_in=ext_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
