{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-docx\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, TRANSFORMERS_CACHE\n",
    "import shutil\n",
    "import fasttext\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from zipfile import BadZipFile\n",
    "import gc\n",
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lang_model = \"./lid218e.bin\"  # path of the pretrained model file\n",
    "if not os.path.isfile(pretrained_lang_model):\n",
    "    # If the file doesn't exist, download it\n",
    "    !wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"NLLB\": \"facebook/nllb-200-3.3B\",\n",
    "    \"MADLAD\": \"google/madlad400-3b-mt\",\n",
    "    \"Llama-3.1-405B\": \"meta-llama/Meta-Llama-3.1-405B\",\n",
    "}\n",
    "\n",
    "def load_model(model_name):\n",
    "    model_dir = f\"{TRANSFORMERS_CACHE}/{model_name}\"\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"{model_name} not found\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        model.save_pretrained(model_dir)\n",
    "        tokenizer.save_pretrained(model_dir)\n",
    "    else:\n",
    "        print(f\"{model_name} found\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"{model_dir}\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(f\"{model_dir}\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def unload_model(model, tokenizer):\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_folder = Path(\"./Input\")\n",
    "output_folder = Path(\"./Translated/\")\n",
    "ext_in = 'docx'\n",
    "ext_out = 'docx'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "output_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_docx(file, translation_pipeline):\n",
    "    \n",
    "    paras = []\n",
    "    # TODO NOTE: this is somewhat odd, because it creates a copy of the input in the output folder. \n",
    "    file = shutil.copy(file, output_folder / \"test\")\n",
    "\n",
    "    # Open connection to Word Document\n",
    "    doc = docx.Document(file)\n",
    "\n",
    "    # read in each paragraph in file and store the style name with it.\n",
    "    for para in doc.paragraphs:\n",
    "        this_para = {'style': para.style.name}\n",
    "        sentences = [sentence for sentence in sent_tokenize(para.text)]\n",
    "        translations = [translation_pipeline(sentence)[0]['translation_text'] for sentence in sentences]\n",
    "\n",
    "        this_para['sentences'] = sentences\n",
    "        this_para['translations'] = translations\n",
    "        paras.append(this_para)\n",
    "\n",
    "        # This line was a great simplification of the find and replace code.\n",
    "        para.text = \" \".join(translations)\n",
    "\n",
    "        # I'm not sure this is required, since the style shouldn't have changed.\n",
    "        para.style = this_para['style']\n",
    "\n",
    "    doc.save(file)\n",
    "    return paras\n",
    "\n",
    "\n",
    "def save_translated_document(file_path, translated_paragraphs):\n",
    "    # Create a new Document\n",
    "    doc = docx.Document()\n",
    "\n",
    "    # Iterate over the translated paragraphs\n",
    "    for para in translated_paragraphs:\n",
    "        # Add a new paragraph with the translated text\n",
    "        new_para = doc.add_paragraph(\" \".join(para['translations']))\n",
    "\n",
    "        # Set the style of the paragraph\n",
    "        new_para.style = para['style']\n",
    "\n",
    "    translated_file_path = file_path.with_suffix('.translated.docx')\n",
    "    # Save the document to the specified file path\n",
    "    doc.save(translated_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages(file):\n",
    "\n",
    "    file = file.resolve()\n",
    "    fasttext_model = fasttext.load_model(pretrained_lang_model)\n",
    "    \n",
    "    # Open the input file as a Word document\n",
    "    try :\n",
    "        document = docx.Document(file)\n",
    "    except BadZipFile:\n",
    "        print(f\"BadZipFile Error on opening {file}\")\n",
    "\n",
    "    paragraphs = [para for para in document.paragraphs]\n",
    "    sentences = [sentence for para in document.paragraphs for sentence in sent_tokenize(para.text)]\n",
    "\n",
    "    languages = Counter()\n",
    "    for sentence in sentences:\n",
    "        predictions = fasttext_model.predict(sentence, k=1)\n",
    "        output_lang = predictions[0][0].replace('__label__', '')\n",
    "        languages.update([output_lang])\n",
    "\n",
    "    del fasttext_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target languages and corresponding file names\n",
    "languages = [\n",
    "    (\"amh_Ethi\", \"Ethiopian\"),\n",
    "    (\"arb_Arab\", \"Arabic\"),\n",
    "    (\"asm_Beng\", \"Assamese\"),\n",
    "    (\"ben_Beng\", \"Bangal\"),\n",
    "    (\"por_Latn\", \"BPortugese\"),\n",
    "    (\"mya_Mymr\", \"Burmese\"),\n",
    "    (\"ceb_Latn\", \"Cebuano\"),\n",
    "    (\"zsm_Latn\", \"Chinese\"),\n",
    "    (\"fra_Latn\", \"French\"),\n",
    "    (\"guj_Gujr\", \"Gujarati\"),\n",
    "    (\"hau_Latn\", \"Hausa\"),\n",
    "    (\"hin_Deva\", \"Hindi\"),\n",
    "    (\"ilo_Latn\", \"Illocano\"),\n",
    "    (\"ind_Latn\", \"Indonesian\"),\n",
    "    (\"kan_Knda\", \"Kannada\"),\n",
    "    (\"khm_Khmr\", \"Khmer\"),\n",
    "    (\"lao_Laoo\", \"Laotian\"),\n",
    "    (\"spa_Latn\", \"LASpanish\"),\n",
    "    (\"mal_Mlym\", \"Malayalam\"),\n",
    "    (\"npi_Deva\", \"Nepali\"),\n",
    "    (\"ory_Orya\", \"Oriya\"),\n",
    "    (\"plt_Latn\", \"PlatMalagasy\"),\n",
    "    (\"pan_Guru\", \"EPunjabi\"),\n",
    "    (\"rus_Cyrl\", \"Russian\"),\n",
    "    (\"swh_Latn\", \"Swahili\"),\n",
    "    (\"tgl_Latn\", \"Tagalog\"),\n",
    "    (\"tam_Taml\", \"Tamil\"),\n",
    "    (\"tel_Telu\", \"Telugu\"),\n",
    "    (\"tha_Thai\", \"Thai\"),\n",
    "    (\"tpi_Latn\", \"TokPisin\"),\n",
    "    (\"urd_Arab\", \"Urdu\"),\n",
    "    (\"vie_Latn\", \"Vietnamese\")\n",
    "]\n",
    "\n",
    "files = [file for file in input_folder.rglob(\"*.\" + ext_in)]\n",
    "print(f\"Found {len(files)} {ext_in} files in {input_folder.resolve()}\")\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "    file = file.resolve()\n",
    "    languages_in_file = get_languages(file)\n",
    "    top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "    file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "\n",
    "    if file_is_english:\n",
    "        print(f\"{i:>4} : Translating file {file} from English to multiple languages.\")\n",
    "        try:\n",
    "            document = docx.Document(file)\n",
    "        except BadZipFile:\n",
    "            print(f\"BadZipFile Error on opening {file}\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        for model_name, checkpoint in checkpoints.items():\n",
    "            print(f\"Loading model: {model_name}\")\n",
    "            model, tokenizer = load_model(checkpoint)\n",
    "\n",
    "            for target_lang, file_name in languages:\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "                translation_pipeline = pipeline('translation',\n",
    "                                                model=model,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                src_lang='eng_Latn',\n",
    "                                                tgt_lang=target_lang,\n",
    "                                                max_length=400,\n",
    "                                                device=device)\n",
    "\n",
    "                paragraphs = translate_docx(file, translation_pipeline)\n",
    "                print(paragraphs)\n",
    "                \n",
    "                output_dir_for_model = output_folder / f\"{model_name}\"\n",
    "\n",
    "                output_dir_for_model.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                output_path = output_dir_for_model / f\"{file.stem}_{file_name}.{ext_out}\"\n",
    "                save_translated_document(output_path, paragraphs)\n",
    "\n",
    "                print(f\"{i:>4} : Translated file {file} to {file_name}.\")\n",
    "                \n",
    "                del translation_pipeline\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            unload_model(model, tokenizer)\n",
    "\n",
    "    else:\n",
    "        print(f\"{i:>4} : Not translating file {file}. It seems to be in :{top_language_in_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
