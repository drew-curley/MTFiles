{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-docx\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, TRANSFORMERS_CACHE\n",
    "import shutil\n",
    "import fasttext\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from zipfile import BadZipFile\n",
    "import gc\n",
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lang_model = \"./lid218e.bin\"  # path of the pretrained model file\n",
    "if not os.path.isfile(pretrained_lang_model):\n",
    "    # If the file doesn't exist, download it\n",
    "    !wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"NLLB\": \"facebook/nllb-200-3.3B\",\n",
    "}\n",
    "\n",
    "def load_model(model_name):\n",
    "    model_dir = f\"{TRANSFORMERS_CACHE}/{model_name}\"\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"{model_name} not found\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        model.save_pretrained(model_dir)\n",
    "        tokenizer.save_pretrained(model_dir)\n",
    "    else:\n",
    "        print(f\"{model_name} found\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"{model_dir}\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(f\"{model_dir}\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def unload_model(model, tokenizer):\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_folder = Path(\"./Input\")\n",
    "output_folder = Path(\"./Translated/\")\n",
    "ext_in = 'docx'\n",
    "ext_out = 'docx'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "output_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_docx(translation_pipeline, input_file, output_file):\n",
    "    \"\"\"Translate a DOCX file and save the translated content to a new file.\"\"\"\n",
    "    doc = docx.Document(input_file)\n",
    "\n",
    "    # Iterate over paragraphs and tables to translate the content\n",
    "    for paragraph in doc.paragraphs:\n",
    "        translate_paragraph(paragraph, translation_pipeline)\n",
    "\n",
    "    for table in doc.tables:\n",
    "        translate_table(table, translation_pipeline)\n",
    "\n",
    "    # Save the translated document\n",
    "    doc.save(output_file)\n",
    "\n",
    "\n",
    "def translate_paragraph(paragraph, translation_pipeline):\n",
    "    \"\"\"Translate the content of a paragraph and replace its text.\"\"\"\n",
    "    original_text = paragraph.text\n",
    "    if original_text.strip():  # Only translate if the paragraph is not empty\n",
    "        translated_text = translate_text(original_text, translation_pipeline)\n",
    "        replace_text_in_runs(paragraph, translated_text)\n",
    "\n",
    "\n",
    "def translate_text(text, translation_pipeline):\n",
    "    \"\"\"Translate the given text using the provided translation pipeline.\"\"\"\n",
    "    return translation_pipeline(text)[0]['translation_text']\n",
    "\n",
    "\n",
    "def replace_text_in_runs(paragraph, translated_text):\n",
    "    \"\"\"Replace text in each run while preserving the original formatting.\"\"\"\n",
    "    original_text = \"\".join(run.text for run in paragraph.runs)\n",
    "\n",
    "    # Ensure we correctly replace text while preserving formatting\n",
    "    if len(original_text) == len(translated_text):\n",
    "        current_char_index = 0\n",
    "        for run in paragraph.runs:\n",
    "            run_length = len(run.text)\n",
    "            run.text = translated_text[current_char_index:current_char_index + run_length]\n",
    "            current_char_index += run_length\n",
    "    else:\n",
    "        # If lengths don't match, replace text by matching run lengths\n",
    "        current_char_index = 0\n",
    "        for run in paragraph.runs:\n",
    "            run_length = len(run.text)\n",
    "            run.text = translated_text[current_char_index:current_char_index + run_length]\n",
    "            current_char_index += run_length\n",
    "\n",
    "        # Handle any leftover text by adding it as a new run\n",
    "        if current_char_index < len(translated_text):\n",
    "            remaining_text = translated_text[current_char_index:]\n",
    "            paragraph.add_run(remaining_text)\n",
    "\n",
    "\n",
    "def translate_table(table, translation_pipeline):\n",
    "    \"\"\"Translate all the cells in a table.\"\"\"\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            for paragraph in cell.paragraphs:\n",
    "                translate_paragraph(paragraph, translation_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages(file):\n",
    "\n",
    "    file = file.resolve()\n",
    "    fasttext_model = fasttext.load_model(pretrained_lang_model)\n",
    "    \n",
    "    # Open the input file as a Word document\n",
    "    try :\n",
    "        document = docx.Document(file)\n",
    "    except BadZipFile:\n",
    "        print(f\"BadZipFile Error on opening {file}\")\n",
    "\n",
    "    paragraphs = [para for para in document.paragraphs]\n",
    "    sentences = [sentence for para in document.paragraphs for sentence in sent_tokenize(para.text)]\n",
    "\n",
    "    languages = Counter()\n",
    "    for sentence in sentences:\n",
    "        predictions = fasttext_model.predict(sentence, k=1)\n",
    "        output_lang = predictions[0][0].replace('__label__', '')\n",
    "        languages.update([output_lang])\n",
    "\n",
    "    del fasttext_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target languages and corresponding file names\n",
    "languages = [\n",
    "    (\"spa_Latn\", \"LASpanish\"),\n",
    "]\n",
    "\n",
    "files = [file for file in input_folder.rglob(\"*.\" + ext_in)]\n",
    "print(f\"Found {len(files)} {ext_in} files in {input_folder.resolve()}\")\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "    file = file.resolve()\n",
    "    # languages_in_file = get_languages(file)\n",
    "    # top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "    # file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "\n",
    "    # if file_is_english:\n",
    "    print(f\"{i:>4} : Translating file {file} from English to multiple languages.\")\n",
    "    try:\n",
    "        document = docx.Document(file)\n",
    "    except BadZipFile:\n",
    "        print(f\"BadZipFile Error on opening {file}\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    for model_name, checkpoint in checkpoints.items():\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        model, tokenizer = load_model(checkpoint)\n",
    "\n",
    "        for target_lang, file_name in languages:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            translation_pipeline = pipeline('translation',\n",
    "                                            model=model,\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            src_lang='eng_Latn',\n",
    "                                            tgt_lang=target_lang,\n",
    "                                            max_length=400,\n",
    "                                            device=device)\n",
    "\n",
    "\n",
    "            \n",
    "            output_dir_for_model = output_folder / f\"{model_name}\"\n",
    "            output_dir_for_model.mkdir(parents=True, exist_ok=True)\n",
    "            output_path = output_dir_for_model / f\"{file.stem}_{file_name}.{ext_out}\"\n",
    "\n",
    "            translate_docx(translation_pipeline, file, output_path)\n",
    "\n",
    "            print(f\"{i:>4} : Translated file {file} to {file_name}.\")\n",
    "            \n",
    "            del translation_pipeline\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        unload_model(model, tokenizer)\n",
    "\n",
    "    # else:\n",
    "    #     print(f\"{i:>4} : Not translating file {file}. It seems to be in :{top_language_in_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
