{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext\n",
    "!pip install -U pip transformers\n",
    "!pip install sentencepiece\n",
    "!pip install python-docx\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import docx\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from nltk import sent_tokenize\n",
    "import torch\n",
    "import fasttext\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'facebook/nllb-200-3.3B'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "pretrained_lang_model = \"./lid218e.bin\"  # path of the pretrained model file\n",
    "if not os.path.isfile(pretrained_lang_model):\n",
    "    # If the file doesn't exist, download it\n",
    "    !wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang = 'eng_Latn'\n",
    "target_lang = 'swh_Latn'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "translation_pipeline = pipeline('translation',\n",
    "                                model=model,\n",
    "                                tokenizer=tokenizer,\n",
    "                                src_lang=input_lang,\n",
    "                                tgt_lang=target_lang,\n",
    "                                max_length = 400,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = Path(os.getenv('INPUT_FOLDER_PATH'))\n",
    "output_folder = Path(os.getenv('OUTPUT_FOLDER_PATH'))\n",
    "ext_in = 'docx'\n",
    "ext_out = 'docx'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "output_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs_from_docx(file):\n",
    "\n",
    "    paras = []\n",
    "    # Open connection to Word Document\n",
    "    doc = docx.Document(file)\n",
    "\n",
    "    # read in each paragraph in file and store the style name with it.\n",
    "    for para in doc.paragraphs:\n",
    "        this_para = {'style': para.style.name}\n",
    "        sentences = []\n",
    "        for sentence in sent_tokenize(para.text):\n",
    "            sentences.append(sentence)\n",
    "        this_para['sentences'] = sentences\n",
    "\n",
    "    return paras\n",
    "\n",
    "def translate_docx(file):\n",
    "    \n",
    "    paras = []\n",
    "    # Copy file to output folder and then work off of the copy\n",
    "    file = shutil.copy(file, output_folder)\n",
    "\n",
    "    # Open connection to Word Document\n",
    "    doc = docx.Document(file)\n",
    "\n",
    "    # read in each paragraph in file and store the style name with it.\n",
    "    for para in doc.paragraphs:\n",
    "        this_para = {'style': para.style.name}\n",
    "        sentences = [sentence for sentence in sent_tokenize(para.text)]\n",
    "        translations = [translation_pipeline(sentence)[0]['translation_text'] for sentence in sentences]\n",
    "\n",
    "        this_para['sentences'] = sentences\n",
    "        this_para['translations'] = translations\n",
    "        paras.append(this_para)\n",
    "\n",
    "        # This line was a great simplification of the find and replace code.\n",
    "        para.text = \" \".join(translations)\n",
    "\n",
    "        # I'm not sure this is required, since the style shouldn't have changed.\n",
    "        para.style = this_para['style']\n",
    "\n",
    "    doc.save(file)\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fasttext_model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "def get_languages(file):\n",
    "\n",
    "    file = file.resolve()\n",
    "\n",
    "    try :\n",
    "        document = docx.Document(file)\n",
    "    except BadZipFile:\n",
    "        print(f\"BadZipFile Error on opening {file}\")\n",
    "\n",
    "    paragraphs = [para for para in document.paragraphs]\n",
    "    sentences = [sentence for para in document.paragraphs for sentence in sent_tokenize(para.text)]\n",
    "\n",
    "    languages = Counter()\n",
    "    for sentence in sentences:\n",
    "        predictions = fasttext_model.predict(sentence, k=1)\n",
    "        output_lang = predictions[0][0].replace('__label__', '')\n",
    "        languages.update([output_lang])\n",
    "\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in input_folder.rglob(\"*.\" + ext_in)]\n",
    "print(f\"Found {len(files)} {ext_in} files in {input_folder.resolve()}\")\n",
    "\n",
    "for i, file in enumerate(files,1):\n",
    "    file = file.resolve()\n",
    "    languages_in_file = get_languages(file)\n",
    "    top_language_in_file = languages_in_file.most_common(1)[0][0]\n",
    "    file_is_english = top_language_in_file == \"eng_Latn\"\n",
    "\n",
    "    if file_is_english:\n",
    "        try :\n",
    "            document = docx.Document(file)\n",
    "        except BadZipFile:\n",
    "            print(f\"BadZipFile Error on opening {file}\")\n",
    "            continue\n",
    "\n",
    "        # Save the file.\n",
    "        document.save(file)\n",
    "\n",
    "        # Translate the content\n",
    "        paragraphs = translate_docx(file)\n",
    "\n",
    "        print(f\"{i:>4} : Translated file {file} from English to Spanish\")\n",
    "    else:\n",
    "        print(f\"{i:>4} : Not translating file {file}. It seems to be in :{top_language_in_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
